{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9df0e1-1cdb-4a55-8958-e1540dfa3caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mint/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mint/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/mint/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pdfplumber\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langdetect import detect\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48396f4-ec25-4385-b291-caf603220980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SentenceTransformer model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Global variables\n",
    "DATASET_PATH = \"\"\n",
    "EMBEDDINGS_FILE = \"document_embeddings.npy\"\n",
    "FAISS_FILE = \"faiss_index.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71b2257d-e3e4-4198-9167-2cf8be2ef3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(documents):\n",
    "    return embedding_model.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    return embedding_model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "def create_faiss_index(embeddings, embedding_dim):\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    np.save(file_path, embeddings)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    return np.load(file_path)\n",
    "\n",
    "def save_faiss_index(index, file_path):\n",
    "    faiss.write_index(index, file_path)\n",
    "\n",
    "def load_faiss_index(file_path):\n",
    "    return faiss.read_index(file_path)\n",
    "\n",
    "def read_pdfs_from_folder(folder_path):\n",
    "    pdf_contents = []\n",
    "    pdf_filenames = []\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Dataset folder '{folder_path}' does not exist.\")\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in '{folder_path}'.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(folder_path, pdf_file)\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \"\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
    "                if text.strip():\n",
    "                    pdf_contents.append(text)\n",
    "                    pdf_filenames.append(pdf_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF file {pdf_file}: {e}\")\n",
    "    \n",
    "    return pdf_contents, pdf_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fbc1c0-7631-47a0-bb8c-c7c2222f6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_nltk_focused(document, query, max_lines=20):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation\n",
    "    from heapq import nlargest\n",
    "    \n",
    "    sentences = sent_tokenize(document)\n",
    "    english_sentences = [s for s in sentences if len(s.strip()) > 10 and detect(s) == 'en']\n",
    "    if not english_sentences:\n",
    "        return \"No relevant English content found related to the query.\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    query_terms = [term.lower() for term in query.split() if term.lower() not in stop_words]\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for sentence in english_sentences:\n",
    "        score = sum(1 for word in word_tokenize(sentence.lower()) if word in query_terms)\n",
    "        if score > 0:\n",
    "            sentence_scores[sentence] = score\n",
    "    \n",
    "    summary_sentences = nlargest(min(len(sentence_scores), max_lines), sentence_scores, key=sentence_scores.get)\n",
    "    return ' '.join(summary_sentences) if summary_sentences else f\"No content specifically about '{query}' found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466f8030-c28b-4634-9a9e-2f741e600503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents, query, num_docs=2):\n",
    "    \"\"\"Process documents and return summaries based on the query.\"\"\"\n",
    "    final_output = []\n",
    "    for doc_idx, document in enumerate(documents[:num_docs]):\n",
    "        summary = summarize_with_nltk_focused(document, query)\n",
    "        if summary and not summary.startswith(\"No content\"):\n",
    "            final_output.append(f\"Document {doc_idx + 1} Response:\\n{summary}\")\n",
    "    \n",
    "    if not final_output:\n",
    "        return f\"No relevant information found about '{query}' in the documents.\"\n",
    "    \n",
    "    return \"\\n\\n\".join(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e57a8b3-e85d-46f1-8f65-7949d1c1c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query():\n",
    "    global DATASET_PATH\n",
    "    query = query_entry.get()\n",
    "    if not query:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a query\")\n",
    "        return\n",
    "    if not DATASET_PATH:\n",
    "        messagebox.showerror(\"Error\", \"Please select a dataset folder\")\n",
    "        return\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response_label.config(text=\"Processing...\")\n",
    "    root.update()\n",
    "    \n",
    "    try:\n",
    "        documents, filenames = read_pdfs_from_folder(DATASET_PATH)\n",
    "        if os.path.exists(EMBEDDINGS_FILE):\n",
    "            document_embeddings = load_embeddings(EMBEDDINGS_FILE)\n",
    "        else:\n",
    "            document_embeddings = embed_documents(documents)\n",
    "            save_embeddings(document_embeddings, EMBEDDINGS_FILE)\n",
    "        \n",
    "        if os.path.exists(FAISS_FILE):\n",
    "            faiss_index = load_faiss_index(FAISS_FILE)\n",
    "        else:\n",
    "            embedding_dim = document_embeddings.shape[1]\n",
    "            faiss_index = create_faiss_index(document_embeddings, embedding_dim=embedding_dim)\n",
    "            save_faiss_index(faiss_index, FAISS_FILE)\n",
    "        \n",
    "        query_embedding = generate_query_embedding(query).reshape(1, -1)\n",
    "        distances, indices = faiss_index.search(query_embedding, k=2)\n",
    "        top_documents = [documents[idx] for idx in indices[0] if idx < len(filenames)]\n",
    "        \n",
    "        final_output = process_documents(top_documents, query) if top_documents else \"No relevant documents found.\"\n",
    "    except Exception as e:\n",
    "        final_output = f\"Error: {str(e)}\"\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    response_label.config(text=final_output)\n",
    "    timer_label.config(text=f\"Time taken: {elapsed_time:.2f} sec\")\n",
    "\n",
    "def select_folder():\n",
    "    global DATASET_PATH\n",
    "    DATASET_PATH = filedialog.askdirectory()\n",
    "    folder_label.config(text=f\"Selected Folder: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30009f9-1226-42d4-9bc6-893847ea7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"NLP Query Interface\")\n",
    "root.geometry(\"500x400\")\n",
    "\n",
    "folder_button = tk.Button(root, text=\"Select Dataset Folder\", command=select_folder)\n",
    "folder_button.pack(pady=5)\n",
    "\n",
    "folder_label = tk.Label(root, text=\"No folder selected\", wraplength=400)\n",
    "folder_label.pack()\n",
    "\n",
    "query_entry = tk.Entry(root, width=50)\n",
    "query_entry.pack(pady=5)\n",
    "query_entry.insert(0, \"Enter your query here\")\n",
    "\n",
    "submit_button = tk.Button(root, text=\"Submit Query\", command=process_query)\n",
    "submit_button.pack(pady=5)\n",
    "\n",
    "response_label = tk.Label(root, text=\"\", wraplength=400)\n",
    "response_label.pack(pady=5)\n",
    "\n",
    "timer_label = tk.Label(root, text=\"\")\n",
    "timer_label.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
